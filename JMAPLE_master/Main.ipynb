{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"11PieBguQ-xxQihVWMUm4Pzoctkp3hmzf","authorship_tag":"ABX9TyMAA8VPInrld0NiKV1Ej40d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Joint MAPLE: Accelerated joint T1 and T2* mapping with scan-specific self-supervised networks\n","\n","MRM link: https://onlinelibrary.wiley.com/doi/10.1002/mrm.29989"],"metadata":{"id":"4WecETjt2TMW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yhU7p2bH2OJc","executionInfo":{"status":"ok","timestamp":1705345234965,"user_tz":-210,"elapsed":9720,"user":{"displayName":"berkin bilgic","userId":"02606955990200443428"}},"outputId":"6f325b4c-ea73-4fbd-f8e8-4f22334c1126"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Maple/02-Second_Project_JMAPLE/GitHub_Pack\n","cuda is available\n"]}],"source":["# imports\n","import torch\n","import torch.optim as optim\n","import numpy as np\n","import scipy.io as sio\n","import sys\n","import os\n","import copy\n","\n","# change the current working directory to the Main.ipynb file directory\n","%cd /content/drive/MyDrive/Maple/02-Second_Project_JMAPLE/GitHub_Pack\n","\n","# import py-modules\n","import utils\n","import parser_ops\n","import unrollednet\n","import relaxsig\n","\n","# define the device\n","if torch.cuda.is_available():\n","    dev = \"cuda:0\"\n","    use_cuda = True\n","    print('cuda is available')\n","else:\n","   dev = \"cpu\"\n","   use_cuda = False\n","parser = parser_ops.get_parser()\n","args = parser"]},{"cell_type":"markdown","source":["# MEMFA Reconstruction Block  : Joint ZS-SSL"],"metadata":{"id":"2EvgIGSA4isj"}},{"cell_type":"code","source":["# Loading data\n","\n","print('Loading data ...')\n","\n","sens_maps =  sio.loadmat(args.data_dir+'cmap_slice_25.mat')['maps'] # multi_channel sensitivity maps -> (nrow, ncol, ncoil)\n","kData = sio.loadmat(args.data_dir+'raw_slice_25.mat')['kData'] # k-space MEMFA data for single slice -> (nrow, ncol, ncoil, nte, nfa)\n","\n","scaler = np.max(np.abs(kData))\n","kspace_train = kData / scaler # scaled k-space data for joint zs_ssl\n","\n","args.nrow_GLOB, args.ncol_GLOB, args.ncoil_GLOB, args.nte_GLOB, args.nfa_GLOB = kspace_train.shape\n","args.ncont_GLOB = args.nte_GLOB * args.nfa_GLOB\n","kspace_train = np.reshape(kspace_train,(args.nrow_GLOB, args.ncol_GLOB, args.ncoil_GLOB, args.ncont_GLOB)) # all contrast in one channel\n","\n","kMask = np.single(np.load (args.data_dir+'kMask_16x_Uniform_complementary.npy')) # uniform 4x4 k-space mask covering all k-space locations\n","original_mask = np.reshape(kMask,(args.nrow_GLOB,args.ncol_GLOB,args.ncont_GLOB)) # k-space mask for joint zs-ssl and contrast-specific\n","\n","# Generate masks\n","cv_trn_mask, cv_val_mask = utils.uniform_selection(kspace_train, original_mask, rho=args.rho_val)\n","\n","remainder_mask, cv_val_mask = np.copy(cv_trn_mask), np.copy(np.complex64(cv_val_mask))\n","\n","print('size of kspace: ', kspace_train[np.newaxis,...].shape, ', sensitivity maps: ', sens_maps.shape, ', masks: ', original_mask.shape)\n","\n","trn_mask, loss_mask = np.empty((args.num_reps, args.nrow_GLOB, args.ncol_GLOB, args.ncont_GLOB), dtype=np.complex64), \\\n","                                np.empty((args.num_reps, args.nrow_GLOB, args.ncol_GLOB, args.ncont_GLOB), dtype=np.complex64)\n","# train data empty arrays\n","nw_input = np.empty((args.num_reps, args.nrow_GLOB, args.ncol_GLOB, args.ncont_GLOB), dtype=np.complex64)\n","ref_kspace = np.empty((args.num_reps, args.nrow_GLOB, args.ncol_GLOB, args.ncoil_GLOB, args.ncont_GLOB), dtype=np.complex64)\n","\n","# validation data empty arrays\n","ref_kspace_val = np.empty((args.num_reps,args.nrow_GLOB, args.ncol_GLOB, args.ncoil_GLOB, args.ncont_GLOB), dtype=np.complex64)\n","nw_input_val = np.empty((args.num_reps, args.nrow_GLOB, args.ncol_GLOB, args.ncont_GLOB), dtype=np.complex64)\n","\n","print('create training & loss masks and generate network inputs... ')\n","# train data\n","for jj in range(args.num_reps):\n","    trn_mask[jj, ...], loss_mask[jj, ...] = utils.uniform_selection(kspace_train,remainder_mask, rho=args.rho_train)\n","\n","    sub_kspace = kspace_train * np.tile(trn_mask[jj][..., np.newaxis,:], (1, 1, args.ncoil_GLOB,1))\n","    ref_kspace[jj, ...] = kspace_train * np.tile(loss_mask[jj][..., np.newaxis,:], (1, 1, args.ncoil_GLOB, 1))\n","    nw_input[jj, ...] = utils.sense1(sub_kspace,sens_maps)\n","\n","# validation data\n","nw_input_val = utils.sense1(kspace_train * np.tile(cv_trn_mask[:, :, np.newaxis,:], (1, 1, args.ncoil_GLOB,1)),sens_maps)[np.newaxis]\n","\n","ref_kspace_val = kspace_train*np.tile(cv_val_mask[:, :, np.newaxis,:], (1, 1, args.ncoil_GLOB,1))[np.newaxis]\n","\n","# prepare data for the training\n","sens_maps_original = np.copy(sens_maps) # keep a copy of the original sensitivity maps\n","sens_maps = np.transpose(sens_maps, (2, 0, 1)) #(nrow,ncol,ncoil) --> (ncoil,nrow,ncol)\n","sens_maps = torch.from_numpy(sens_maps).to(dev)\n","ref_kspace = utils.complex2real(np.transpose(ref_kspace, (0, 3, 1, 2, 4))) #(batch, ncoil, nrow, ncol, ncont)\n","\n","nw_input = utils.complex2real(nw_input)\n","\n","ref_kspace_val = utils.complex2real(np.transpose(ref_kspace_val, (0, 3, 1, 2, 4)))\n","\n","nw_input_val = utils.complex2real(nw_input_val)\n","\n","print('size of ref kspace: ', ref_kspace.shape, ', nw_input: ', nw_input.shape, ', sensitivity maps: ', sens_maps.shape, ', masks: ', trn_mask.shape)\n","\n","total_batch = int(np.floor(np.float32(nw_input.shape[0]) / (args.batchSize))) # set the batch size\n","\n","trn_mask = torch.from_numpy(trn_mask)\n","loss_mask = torch.from_numpy(loss_mask)\n","cv_val_mask = torch.from_numpy(cv_val_mask)\n","cv_trn_mask = torch.from_numpy(cv_trn_mask)\n","\n","epoch, val_loss_tracker = 0, 0\n","trn_loss = 0\n","model = unrollednet.UnrolledNet(sens_maps).to(dev)\n","\n","if args.transfer_learning: # transfer learning\n","   model.load_state_dict(torch.load(args.model_dir+'saved_weights.pth'))\n","   model.train()\n","\n","total_loss = []\n","total_val_loss = []\n","scalar = torch.tensor([0.5], dtype=torch.float32).to(dev)\n","optimizer = torch.optim.Adam(model.parameters(), lr = args.zs_ssl_lr)\n","lowest_val_loss = np.inf\n","nw_input = torch.from_numpy(nw_input)\n","\n","ref_kspace = torch.from_numpy(utils.real2complex(ref_kspace))\n","ref_kspace_val = torch.from_numpy(utils.real2complex(ref_kspace_val))\n","\n","nw_input_val = torch.tensor(nw_input_val, dtype=torch.float32)\n","\n","print(\"NUmber of trainable parameters is:\")\n","model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","params = sum([np.prod(p.size()) for p in model_parameters])\n","print(params)\n","\n","# training\n","while epoch < args.epochs and val_loss_tracker < args.stop_training:\n","    avg_cost = 0\n","    epoch_loss = 0\n","    for j in range(total_batch):\n","        if nw_input.shape[0] - j * args.batchSize < args.batchSize:\n","            current_batch = nw_input[j * args.batchSize:].to(dev)\n","            current_kspace = ref_kspace[j * args.batchSize:].to(dev)\n","            model.set_trn_mask(trn_mask[j * args.batchSize:].to(dev))\n","            model.set_loss_mask(loss_mask[j * args.batchSize:].to(dev))\n","            nw_output_image, nw_output_kspace, *_ = model.forward(current_batch, epoch)\n","\n","        else:\n","            current_batch = nw_input[j * args.batchSize : (j+1) * args.batchSize].to(dev)\n","            current_kspace = ref_kspace[j * args.batchSize : (j+1) * args.batchSize].to(dev)\n","            model.set_trn_mask(trn_mask[j * args.batchSize : (j+1) * args.batchSize].to(dev))\n","            model.set_loss_mask(loss_mask[j * args.batchSize : (j+1) * args.batchSize].to(dev))\n","            nw_output_image, nw_output_kspace, *_ = model.forward(current_batch, epoch)\n","        trn_loss = torch.mul(scalar, torch.linalg.norm(nw_output_kspace - current_kspace)) / torch.linalg.norm(current_kspace) + torch.mul(scalar, torch.linalg.norm(torch.flatten(current_kspace-nw_output_kspace), ord=1))  / torch.linalg.norm(torch.flatten(current_kspace),ord=1)\n","\n","        epoch_loss += trn_loss\n","        optimizer.zero_grad()\n","        trn_loss.backward()\n","        optimizer.step()\n","    print(f\"Training loss in epoch {epoch} is {epoch_loss.item()}\")\n","\n","    with torch.no_grad():\n","        model.set_trn_mask(torch.unsqueeze(cv_trn_mask, 0).to(dev))\n","        model.set_loss_mask(torch.unsqueeze(cv_val_mask, 0).to(dev))\n","        nw_input_val = nw_input_val.to(dev)\n","        val_output_kspace = model.forward(nw_input_val,epoch)[1]\n","        val_loss = torch.mul(scalar, torch.linalg.norm(val_output_kspace - ref_kspace_val.to(dev)) / torch.linalg.norm(ref_kspace_val.to(dev)) + torch.mul(scalar, torch.linalg.norm(torch.flatten(ref_kspace_val.to(dev)-val_output_kspace), ord=1)) / torch.linalg.norm(torch.flatten(ref_kspace_val.to(dev)),ord=1))\n","        print(f\"Validation loss in epoch {epoch} is {val_loss.item()}\")\n","        total_val_loss.append(val_loss)\n","        if val_loss <= lowest_val_loss:\n","            lowest_val_loss = val_loss\n","            val_loss_tracker = 0\n","            torch.save(model.state_dict(), args.model_dir+'saved_weights.pth')\n","        else:\n","            val_loss_tracker += 1\n","        epoch += 1\n","\n","print(\"training of joint zs-ssl network is finished\")\n","print(\"MEMFA reconstruction using trained network ...\")\n","\n","# MEMFA reconstruction using trained network\n","\n","test_maskc = torch.tensor(original_mask[None]).to(dev)\n","model = unrollednet.UnrolledNet(sens_maps, test_maskc, test_maskc).to(dev)\n","model.load_state_dict(torch.load(args.model_dir+'saved_weights.pth'))\n","model.eval()\n","\n","nw_input = utils.sense1(kspace_train * np.tile(original_mask[..., np.newaxis,:], (1, 1, args.ncoil_GLOB,1)),sens_maps_original)\n","nw_input = torch.from_numpy(utils.complex2real(nw_input)).type(torch.FloatTensor)\n","nw_input = torch.unsqueeze(nw_input, 0).to(dev)\n","zs_ssl_recon, _, _ = model.forward(nw_input.to(dev))\n","zs_ssl_recon = (utils.real2complex(zs_ssl_recon.squeeze())*scaler).detach().cpu().numpy()\n","zs_ssl_recon_final = np.reshape(zs_ssl_recon,(args.nrow_GLOB,args.ncol_GLOB,args.nte_GLOB,args.nfa_GLOB))\n","np.save(args.recon_dir+'joint_zs_ssl_recon_slice_25.npy',zs_ssl_recon_final)\n","\n","print(\"joint zs-ssl reconstruction is finished\")\n","\n","#utils.display_all(abs(zs_ssl_recon_final))"],"metadata":{"id":"nWfbrcmOENEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Parameter Mapping"],"metadata":{"id":"xiVrXZTz68S4"}},{"cell_type":"code","source":["# define necessary in-script functions\n","def T1_Mapping(lr,epochs,target, datc, map_init):\n","\n","    model_param = relaxsig.T1_Signal(map_init, 1e-1)\n","    model_param.set_constants(ang_use, TR, B1, phase_T1)\n","    model_param = model_param.to(dev)\n","\n","    optimizer = optim.Adam(model_param.parameters(), lr=lr)\n","\n","    for k in range(epochs):\n","        optimizer.zero_grad()\n","        a = model_param()\n","        loss = utils.MSEc(a, target)\n","\n","        loss.backward()\n","        optimizer.step()\n","    print('T1 initialization finished after {} epochs.'.format(epochs))\n","\n","    return model_param\n","\n","def T2_Mapping(lr,epochs,target, datc, map_init):\n","\n","    model_param = relaxsig.T2_Signal(map_init, 1e-10)\n","    model_param.set_te(TE_use)\n","    model_param = model_param.to(dev)\n","\n","    optimizer = optim.Adam(model_param.parameters(), lr=lr)\n","\n","    for k in range(epochs):\n","        optimizer.zero_grad()\n","        a = model_param()\n","        loss = utils.MSEc(a, target)\n","\n","        loss.backward()\n","        optimizer.step()\n","    print('T2 initialization finished after {} epochs.'.format(epochs))\n","\n","    return model_param\n","\n","def Joint_Mapping_Init(lr,epochs,target, map_init):\n","\n","    model_param = relaxsig.Joint_Signal(map_init, 1e0)\n","    model_param.set_constants(ang_use, TE_use, TR, B1, phase)\n","    model_param = model_param.to(dev)\n","\n","    optimizer = optim.Adam(model_param.parameters(), lr=lr)\n","\n","    k = 0\n","    while k < epochs:\n","        optimizer.zero_grad()\n","        a = model_param()\n","        loss = utils.MSEc(a, target)\n","\n","        if(k+1)%1000 == 0:\n","\n","          print('iter', k+1, 'train loss: ' + str(loss.cpu().detach().numpy()))\n","\n","        k += 1\n","        loss.backward()\n","        optimizer.step()\n","\n","    print('joint initialization finished after {} epochs.'.format(epochs))\n","\n","    return model_param\n","\n","def Joint_Mapping(lr,epochs,target,mu, datc, map_init, scaler):\n","\n","    model_param = relaxsig.Joint_Signal(map_init, 1e0)\n","    model_param.set_constants(ang_use, TE_use, TR, B1, phase)\n","    model_param = model_param.to(dev)\n","\n","    optimizer = optim.Adam(model_param.parameters(), lr=lr)\n","\n","    k=0\n","    Gen_1 = np.inf\n","    T1_nrmse_min, T2_nrmse_min = np.inf, np.inf\n","    while k < epochs:\n","\n","        optimizer.zero_grad()\n","        a=model_param()\n","\n","        loss1 = utils.MSEc(a, target)\n","        loss2 = utils.MSEc(utils.Ac_Combined(a.permute(3,2,0,1)[None],maskc,coilc),datc)\n","        loss = loss1 + mu*loss2\n","\n","        if(k+1)%100 == 0:\n","\n","          T1 = 1/model_param.R1.detach().squeeze().cpu().numpy()\n","          T2 = 1/model_param.R2.detach().squeeze().cpu().numpy()\n","          delB = model_param.delB.detach().squeeze().cpu().numpy()\n","          Gen = model_param().detach().squeeze().cpu().numpy()*scaler\n","\n","          T1_nrmse = utils.NRMSE(T1,T1_ideal,brain_mask,up_sat = 2500,dw_sat = 0)\n","          T2_nrmse = utils.NRMSE(T2,T2_ideal,brain_mask,up_sat = 100, dw_sat = 0)\n","          delB_nrmse = utils.NRMSE(delB,delB_ideal,brain_mask)\n","          Gen_NRMSE = utils.NRMSE(Gen,ideal,brain_mask)\n","\n","          if T1_nrmse <= T1_nrmse_min:\n","             T1_nrmse_min = T1_nrmse\n","             T1_best_model = copy.deepcopy(model_param)\n","\n","          if T2_nrmse <= T2_nrmse_min:\n","             T2_nrmse_min = T2_nrmse\n","             T2_best_model = copy.deepcopy(model_param)\n","\n","          print('------------------------------')\n","          print('iter', k+1, 'train loss: ' + str(loss.cpu().detach().numpy()))\n","          print('T1 NRMSE= {}'.format(T1_nrmse))\n","          print('T2 NRMSE= {}'.format(T2_nrmse))\n","          print('delB NRMSE= {}'.format(delB_nrmse))\n","          print('Gen NRMSE= {}'.format(Gen_NRMSE))\n","          print('Best T1 NRMSE:{}'.format(T1_nrmse_min))\n","          print('Best T2 NRMSE:{}'.format(T2_nrmse_min))\n","\n","          if Gen_1 - Gen_NRMSE <= args.Tol:\n","             break\n","          else:\n","               Gen_1 = Gen_NRMSE\n","        k += 1\n","        loss.backward()\n","        optimizer.step()\n","\n","    return T1_best_model, T2_best_model, model_param\n","\n","\n","# data preparation for parameter mapping\n","coil_sens = sens_maps_original[:,:,:,None,None]\n","coilc = torch.tensor(coil_sens.transpose(2,4,3,0,1),dtype=torch.cfloat)\n","\n","kMask = kMask[:,:,None,:,:] # contrast-specific k-space mask\n","\n","datc=torch.tensor((kMask*kData).transpose(2,4,3,0,1),dtype=torch.cfloat) # sub-sampled k-space data (ncoil, nfa, nte, nrow, ncol)\n","\n","maskc = torch.tensor(kMask.transpose(2,4,3,0,1))\n","\n","coilc, datc, maskc = coilc.to(dev), datc.to(dev), maskc.to(dev)\n","\n","brain_mask = np.load(args.data_dir+'brain_mask_slice_25.npy') # for error calculations\n","skull_mask = np.load(args.data_dir+'skull_mask_slice_25.npy') # for maps show\n","\n","ang = np.arange(4,17,6) # sequence parameters: three flip angles\n","ang = (ang*np.pi)/180\n","ang_use = torch.tensor(ang[None,None,None,:]).float()\n","\n","TE = np.arange(3.6,29,5) # sequence parameters: six echo times\n","TE_use = torch.tensor(TE[None,None,:,None]).float()\n","TR = 34\n","TE_use, ang_use = TE_use.to(dev), ang_use.to(dev)\n","\n","# reconstruction of ideal image for calculation of the error for generated synthetic image\n","ideal = (coil_sens.conj()*utils.ift2_np(kData)).sum(2) / ((abs(coil_sens)**2).sum(2)+np.finfo(float).eps);\n","\n","B1 = sio.loadmat(args.data_dir+'b1_slc_25.mat')['Output'] # B1 map\n","b1 = np.flipud(B1)\n","C1 = np.copy(b1) # handling the negative stride\n","B1 = torch.tensor(C1[...,None,None],dtype=torch.float).to(dev)\n","\n","# golden fully-sampled maps as the ground truths\n","T1_ideal = np.load(args.golden_dir+'T1_Golden_slc_25.npy')\n","T2_ideal = np.load(args.golden_dir+'T2_Golden_slc_25.npy')\n","delB_ideal = np.load(args.golden_dir+'delB_Golden_slc_25.npy')\n","\n","# acceleration rate demonstration\n","acc = np.prod(kMask.shape) / kMask.sum()\n","acc = int(np.round(acc))\n","print('acc: ' + str(acc) + 'x')\n","\n","recon=np.load(args.recon_dir+'joint_zs_ssl_recon_slice_25.npy') # MEMFA reconstructed image from Recon_Block\n","\n","# parameter estination setting\n","mapping_scaler = np.max(abs(recon))\n","mapping_scaler_T1 = np.max(abs(recon[...,0,:])) # TE = 3.6ms selected\n","mapping_scaler_T2 = np.max(abs(recon[...,:,1])) # FA = 10 degree selsected\n","\n","target = torch.tensor(recon,dtype=torch.cfloat)\n","target_T1 = target[...,0,:]/mapping_scaler_T1 # TE = 3.6ms selected\n","target_T2 = target[...,:,1]/mapping_scaler_T2 # FA = 10 degree selsected\n","target = target/mapping_scaler\n","\n","phase = torch.exp(1j*torch.tensor(np.angle(target),dtype = torch.cfloat).to(dev)) # phase estimation for joint signal model\n","phase_T1 = torch.exp(1j*torch.tensor(np.angle(target_T1),dtype = torch.cfloat).to(dev)) # phase estimation for T1 signal model\n","\n","target, target_T1, target_T2 = target.to(dev), target_T1.to(dev), target_T2.to(dev)\n","\n","# first step of the fast initialization with individual signal models and conventional parameter fitting\n","print('first step of initialization with individual signal models ..')\n","init_model_T1 = T1_Mapping(1e-3,500, target_T1, datc[:,:,0,...], [torch.rand(args.nrow_GLOB,args.ncol_GLOB,1,2),\n","                                    torch.rand(args.nrow_GLOB,args.ncol_GLOB,1)])\n","\n","\n","init_model_T2 = T2_Mapping(1e-3, 3000, target_T2, datc[:,0,...], [torch.rand(args.nrow_GLOB,args.ncol_GLOB,1,2),\n","                                    torch.rand(args.nrow_GLOB,args.ncol_GLOB,1),\n","                                    torch.rand(args.nrow_GLOB,args.ncol_GLOB,1)])\n","\n","# second step of initialization with joint signal model and conventional parameter fitting\n","print('second step of initialization with joint signal model ..')\n","init_model_joint = Joint_Mapping_Init(args.init_LR,args.init_Epochs, target,[init_model_T2.M0.unsqueeze(3),\n","                                    init_model_T1.R1.unsqueeze(-1),\n","                                    init_model_T2.R2.unsqueeze(-1),\n","                                    init_model_T2.delB.unsqueeze(-1)])\n","\n","# Joint MAPLE parameter mapping with loss1 and loss2\n","print('Joint MAPLE parameter mapping ..')\n","T1_model, T2_model, model_comb = Joint_Mapping(args.JM_LR,args.JM_Epochs, target, args.MU,\n","                                    datc/mapping_scaler,[init_model_joint.M0,\n","                                     init_model_joint.R1,init_model_joint.R2,\n","                                     init_model_joint.delB], scaler = mapping_scaler)\n","\n","# output maps\n","M0_est = torch.view_as_complex(T1_model.M0.detach()).squeeze()\n","R1_est = T1_model.R1.detach().squeeze()\n","R2_est = T2_model.R2.detach().squeeze()\n","delB_est = T2_model.delB.detach().squeeze()\n","gen_img_Gen = model_comb().detach()*mapping_scaler\n","\n","if use_cuda:\n","    M0_est, R1_est, R2_est, delB_est = M0_est.cpu(), R1_est.cpu(), R2_est.cpu(), delB_est.cpu()\n","\n","M0_est, R1_est, R2_est, delB_est = M0_est.numpy(), R1_est.numpy(), R2_est.numpy(), delB_est.numpy()\n","\n","T1_est, T2_est = 1/R1_est, 1/R2_est\n","\n","print('Mapping Process Finished!')\n","\n","# save estimated parameter maps\n","np.save(args.param_dir+'M0_slc_25_R16cus',M0_est)\n","np.save(args.param_dir+'T1_slc_25_R16cus',T1_est)\n","np.save(args.param_dir+'T2_slc_25_R16cus',T2_est)\n","np.save(args.param_dir+'delB_slc_25_R16cus',delB_est)\n","np.save(args.param_dir+'gen_img_slc_25_R16cus',gen_img_Gen.cpu().numpy())\n","\n","utils.display_maps(M0_est,T1_est,T2_est, delB_est, skull_mask)\n"],"metadata":{"id":"1k5jX0-REPA0"},"execution_count":null,"outputs":[]}]}